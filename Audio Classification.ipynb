{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING PACKAGES\n",
    "\n",
    "import struct\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Add\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7\n",
    "# HELPER FUNCTION FOR READING AUDIO FILE\n",
    "class WavFileHelper():\n",
    "    # read the files in the UrbanSound8K according to the CSV file contained in the folder\n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename,\"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n",
    "        # return the channel, smaple rate and bit depth information of the sound files\n",
    "        return (num_channels, sample_rate, bit_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACTING THE WAVE FORM FROM SOUND TRACK AND CONVERT TO 1-D ARRAY\n",
    "# n is the mfcc value\n",
    "def extract_feature(file_name, n):  \n",
    "    standard_size = 88200  # standard size for audio signal which is about 2 s\n",
    "    try:\n",
    "        audio, sample_rate = librosa.core.load(file_name, mono=True, res_type='kaiser_fast') \n",
    "        fill = standard_size - audio.shape[0]\n",
    "        # if the sound file is less than 2s, fill the short part with zeros\n",
    "        if(fill>0):\n",
    "            audio = np.concatenate((audio, np.zeros(fill)), axis=0)\n",
    "        # if the file is more than 2s, clip the excess part \n",
    "        elif(fill<0):\n",
    "            audio = audio[:standard_size]\n",
    "            \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n)\n",
    "        mfccs = librosa.util.normalize(mfccs)  \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "    # return the audio spectrum of the sound file\n",
    "    return mfccs\n",
    "\n",
    "    \n",
    "    \n",
    "# Set the path to the full UrbanSound dataset \n",
    "def audio_extration(n=40):\n",
    "    fulldatasetpath = 'UrbanSound8K/audio/' \n",
    "\n",
    "    metadata = pd.read_csv(fulldatasetpath + 'metadata/UrbanSound8K.csv')\n",
    "\n",
    "    features = []\n",
    "    label_amount = {}\n",
    "\n",
    "    # Iterate through each sound file and extract the features \n",
    "    for index, row in metadata.iterrows():\n",
    "    \n",
    "        file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "        class_label = row[\"class\"]\n",
    "        data = extract_feature(file_name, n)\n",
    "        label_amount[class_label] = label_amount.get(class_label, 0) + 1\n",
    "        features.append([data, class_label])\n",
    "\n",
    "\n",
    "    # Convert into a Panda dataframe \n",
    "    featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "    #print('Finished feature extraction from ', len(featuresdf), ' files')\n",
    "    return featuresdf, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT THE CHANNELS, SAMPLE RATE AND BIT DEPTH DATA FROM SOUNDS FILE\n",
    "# use the previous functinons and actually do the info extraction\n",
    "def info_extration(metadata):\n",
    "    wavfilehelper = WavFileHelper()\n",
    "\n",
    "    audiodata = []\n",
    "    for index, row in metadata.iterrows():\n",
    "    \n",
    "        file_name = os.path.join(os.path.abspath('UrbanSound8K/audio/'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "        data = wavfilehelper.read_file_properties(file_name)\n",
    "        audiodata.append(data)\n",
    "\n",
    "    # Convert into a Panda dataframe\n",
    "    audiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])\n",
    "    return audiodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is used to test the previous functions\n",
    "'''\n",
    "print(type(featuresdf.iloc[0]['feature']))\n",
    "print(featuresdf.head())\n",
    "print()\n",
    "print(audiodf.head())\n",
    "\n",
    "lst = [0,1,10,23,96,106,114,122,171,196]\n",
    "for n in lst:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline  \n",
    "    plt.plot(featuresdf.iloc[n]['feature'][5:])\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    print(featuresdf.iloc[n]['class_label'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPERATING DATA TO TRAINING AND TESTING WITH 80% 20% RATIO\n",
    "# use the previous functions and actually read and store all the files in the dataset\n",
    "def data_seperation(featuresdf):\n",
    "    # Convert features and corresponding classification labels into numpy arrays\n",
    "    X = np.array(featuresdf.feature.tolist())\n",
    "    y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "    # Encode the labels using Onehot technique\n",
    "    lable_encoder = LabelEncoder()\n",
    "    y_one_hot = to_categorical(lable_encoder.fit_transform(y)) \n",
    "\n",
    "    # split the dataset \n",
    "    train_features, validate_features, train_labels, validate_labels = train_test_split(X, y_one_hot, test_size=0.11, random_state = 42)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(train_features, train_labels, test_size=0.12, random_state = 42)\n",
    "    #print(\"Training data set size: \",train_features.shape[0])\n",
    "    #print(\"Validate data set size: \",validate_features.shape[0])\n",
    "    #print(\"Test data set size: \",test_features.shape[0])\n",
    "    return (train_features, validate_features, test_features, train_labels, validate_labels, test_labels, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for easy access\n",
    "def save_data(n, train_features, validate_features, test_features, train_labels, validate_labels, test_labels, y_one_hot):\n",
    "    pickle_file = 'audio_features_mfcc'+ str(n) +'.pickle'\n",
    "    if not os.path.isfile(pickle_file):\n",
    "        print('Saving data to pickle file...')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as pfile:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        'train_dataset': train_features,\n",
    "                        'train_labels': train_labels,\n",
    "                        'valid_dataset': validate_features,\n",
    "                        'valid_labels': validate_labels,\n",
    "                        'test_dataset': test_features,\n",
    "                        'test_labels': test_labels,\n",
    "                        'y_one_hot': y_one_hot,\n",
    "                    },\n",
    "                    pfile, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL BUILDING\n",
    "# n is mfcc number\n",
    "def model_building(n, validate_features, validate_labels, y_one_hot):\n",
    "    num_rows = n  # audio frequency spectrum 173*n\n",
    "    num_columns = 173   \n",
    "    num_channels = 1   # combine 2 channels to one channel\n",
    "    \n",
    "    #validate_features = validate_features.reshape(validate_features.shape[0], num_rows, num_columns, num_channels) \n",
    "    num_labels = y_one_hot.shape[1]\n",
    "    \n",
    "    filter_size = 2   \n",
    "    drop_rate = 0.4  # dropping 40% of data to prevent over-fitting\n",
    "\n",
    "    # Construct model \n",
    "    model = Sequential()\n",
    "    # Layer 1 - Conv Layer 1\n",
    "    model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # Layer  - Conv Layer 2\n",
    "    model.add(Conv2D(filters=32, kernel_size=2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # Layer 3 - Conv Layer 3\n",
    "    model.add(Conv2D(filters=64, kernel_size=2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # Layer 4 - Conv Layer 4\n",
    "    model.add(Conv2D(filters=128, kernel_size=2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # Layer 5 - Flatten Layer\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "    # Pre-training evaluation\n",
    "    \n",
    "    # Display model architecture summary \n",
    "    # model.summary()\n",
    "\n",
    "    # Calculate pre-training accuracy \n",
    "    # score = model.evaluate(validate_features, validate_labels, verbose=1)\n",
    "    # accuracy = 100*score[1]\n",
    "\n",
    "    # print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def model_training(n, model, train_features, train_labels, validate_features, \n",
    "                   validate_labels, num_epochs=150, num_batch_size=256):\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', verbose=1, save_best_only=True)\n",
    "    \n",
    "    num_rows = n  # audio frequency spectrum 173*40\n",
    "    num_columns = 173   \n",
    "    num_channels = 1   # combine 2 channels to one channel\n",
    "    train_features = train_features.reshape(train_features.shape[0], num_rows, num_columns, num_channels) \n",
    "    validate_features = validate_features.reshape(validate_features.shape[0], num_rows, num_columns, num_channels)   \n",
    "    \n",
    "    #start = datetime.now()\n",
    "\n",
    "    history = model.fit(train_features, train_labels, batch_size=num_batch_size, epochs=num_epochs, validation_data=(validate_features, validate_labels), shuffle=True, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    #duration = datetime.now() - start\n",
    "    #print(\"Training completed in time: \", duration)\n",
    "    return history\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MobileNet model\n",
    "def mobileNet_building(n, validate_features, validate_labels, y_one_hot):\n",
    "    num_rows = n  # audio frequency spectrum n*173\n",
    "    num_columns = 173   \n",
    "    num_channels = 3 \n",
    "    \n",
    "    drop_rate = 0.4  # dropping 40% of data to prevent over-fitting\n",
    "              \n",
    "    input_img = Input(shape=(num_rows, num_columns, num_channels))\n",
    "    mn = MobileNetV2(input_shape=(num_rows, num_columns, num_channels), include_top=False, weights='imagenet',input_tensor=input_img, pooling='max')\n",
    "\n",
    "    # add a layer at the end of the model\n",
    "    # the original MobileNet has 1000 classes, here only have 10 classes\n",
    "    # so add a layer with 10 output at the end\n",
    "    model = Sequential()\n",
    "    model.add(mn)\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the MobileNet model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    model.summary()\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def mobileNet_training(n, model, train_features, train_labels, validate_features, \n",
    "                   validate_labels, num_epochs=150, num_batch_size=256):\n",
    "    \n",
    "    #start = datetime.now()\n",
    "\n",
    "    history = model.fit(train_features, train_labels, batch_size=num_batch_size, epochs=num_epochs, validation_data=(validate_features, validate_labels), shuffle=True, verbose=1)\n",
    "\n",
    "    #duration = datetime.now() - start\n",
    "    #print(\"Training completed in time: \", duration)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle file to \n",
    "def load_data(pickle_file_path):\n",
    "    file_dict = pickle.load( open( pickle_file_path, \"rb\" ))\n",
    "    return file_dict['train_dataset'], file_dict['valid_dataset'],\\\n",
    "file_dict['test_dataset'],file_dict['train_labels'],\\\n",
    "file_dict['valid_labels'], file_dict['test_labels'], file_dict['y_one_hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe Line\n",
    "# experimenting mfcc number\n",
    "def pip(n):\n",
    "    # data preprocessing\n",
    "    '''\n",
    "    featuresdf, metadata = audio_extration(n)\n",
    "    audiodf = info_extration(metadata)\n",
    "    train_features, validate_features, test_features, \\\n",
    "    train_labels, validate_labels, test_labels, y_one_hot = data_seperation(featuresdf)\n",
    "    save_data(n, train_features, validate_features, test_features, train_labels, validate_labels, test_labels, y_one_hot)\n",
    "    '''\n",
    "    # load pre-processed data\n",
    "    pickle_file_path = 'audio_features_mfcc40.pickle'\n",
    "    train_features, validate_features, test_features, train_labels,\\\n",
    "    validate_labels, test_labels, y_one_hot = load_data(pickle_file_path)\n",
    "    \n",
    "    # Simple model\n",
    "    model = model_building(n, validate_features, validate_labels, y_one_hot)\n",
    "    history = model_training(n, model, train_features, train_labels, validate_features, \n",
    "                             validate_labels, num_epochs=150, num_batch_size=256)\n",
    "    # Evaluating the model on the training and testing set\n",
    "\n",
    "    score = model.evaluate(train_features, train_labels, verbose=0)\n",
    "    print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "    score = model.evaluate(valid_features, test_labels, verbose=0)\n",
    "    print(\"Validation Accuracy: \", score[1])\n",
    "    \n",
    "    \n",
    "    # MobileNet\n",
    "    #train_features = np.stack((train_features, train_features, train_features), axis=3)\n",
    "    \n",
    "    #validate_features = np.stack((validate_features, validate_features, validate_features), axis=3)\n",
    "    #print(train_features.shape)\n",
    "    #print(validate_features.shape)\n",
    "    #model = mobileNet_building(n, validate_features, validate_labels, y_one_hot)\n",
    "    #history = mobileNet_training(n, model, train_features, train_labels, validate_features, \n",
    "    #                         validate_labels, num_epochs=50, num_batch_size=256)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6838 samples, validate on 961 samples\n",
      "Epoch 1/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 2.1950 - acc: 0.1676 - val_loss: 2.1446 - val_acc: 0.2279\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.14464, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/150\n",
      "6838/6838 [==============================] - 11s 2ms/step - loss: 1.9798 - acc: 0.2723 - val_loss: 1.9321 - val_acc: 0.3049\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.14464 to 1.93206, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/150\n",
      "6838/6838 [==============================] - 11s 2ms/step - loss: 1.8367 - acc: 0.3184 - val_loss: 1.8256 - val_acc: 0.3538\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.93206 to 1.82564, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.7272 - acc: 0.3655 - val_loss: 1.7138 - val_acc: 0.4100\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.82564 to 1.71378, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.6322 - acc: 0.4125 - val_loss: 1.5979 - val_acc: 0.4339\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.71378 to 1.59789, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.5418 - acc: 0.4498 - val_loss: 1.5239 - val_acc: 0.4568\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.59789 to 1.52388, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/150\n",
      "6838/6838 [==============================] - 11s 2ms/step - loss: 1.4755 - acc: 0.4784 - val_loss: 1.4480 - val_acc: 0.5057\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.52388 to 1.44799, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 8/150\n",
      "6838/6838 [==============================] - 11s 2ms/step - loss: 1.4048 - acc: 0.5034 - val_loss: 1.3976 - val_acc: 0.5307\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.44799 to 1.39758, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.3668 - acc: 0.5231 - val_loss: 1.3554 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.39758 to 1.35537, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 10/150\n",
      "6838/6838 [==============================] - 13s 2ms/step - loss: 1.3281 - acc: 0.5313 - val_loss: 1.3101 - val_acc: 0.5546\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.35537 to 1.31008, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.2975 - acc: 0.5436 - val_loss: 1.2828 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.31008 to 1.28281, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 12/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.2741 - acc: 0.5486 - val_loss: 1.2564 - val_acc: 0.5713\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.28281 to 1.25642, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 13/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.2381 - acc: 0.5674 - val_loss: 1.2130 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.25642 to 1.21298, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 14/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.2117 - acc: 0.5693 - val_loss: 1.1936 - val_acc: 0.5921\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.21298 to 1.19360, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 15/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.1967 - acc: 0.5798 - val_loss: 1.1812 - val_acc: 0.6056\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.19360 to 1.18120, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 16/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.1754 - acc: 0.5888 - val_loss: 1.1610 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.18120 to 1.16101, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 17/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.1470 - acc: 0.6005 - val_loss: 1.1415 - val_acc: 0.6087\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.16101 to 1.14154, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 18/150\n",
      "2048/6838 [=======>......................] - ETA: 8s - loss: 1.1310 - acc: 0.5957"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-2f46d3ed353f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhistory_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmfcc_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mhistory_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3f5c091116a1>\u001b[0m in \u001b[0;36mpip\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_building\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     history = model_training(n, model, train_features, train_labels, validate_features, \n\u001b[0;32m---> 20\u001b[0;31m                              validate_labels, num_epochs=150, num_batch_size=256)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# MobileNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8f29724cf455>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(n, model, train_features, train_labels, validate_features, validate_labels, num_epochs, num_batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#start = datetime.now()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#duration = datetime.now() - start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate the effect of different MFCC number vs model accuracy\n",
    "mfcc_num = list(range(40,260,20))\n",
    "accuracy = []\n",
    "history_list = []\n",
    "for n in mfcc_num:\n",
    "    history = pip(n)\n",
    "    history_list.append(history)\n",
    "    accuracy.append(history.history['val_acc'][-1])\n",
    "\n",
    "plt.plot(mfcc_num, accuracy)\n",
    "plt.title('Model Accuracy vs. MFCC Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('MFCC Number')\n",
    "plt.savefig('Model Accuracy vs. MFCC Number.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history saved to pickle file.\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'training_history.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open(pickle_file, 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                        'history_list': history_list,\n",
    "\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Training history saved to pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6838 samples, validate on 961 samples\n",
      "Epoch 1/150\n",
      "6838/6838 [==============================] - 13s 2ms/step - loss: 2.2076 - acc: 0.1481 - val_loss: 2.1779 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17792, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 2.0178 - acc: 0.2675 - val_loss: 1.9808 - val_acc: 0.3278\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.17792 to 1.98083, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/150\n",
      "6838/6838 [==============================] - 14s 2ms/step - loss: 1.8135 - acc: 0.3472 - val_loss: 1.7730 - val_acc: 0.3954\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.98083 to 1.77296, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.6801 - acc: 0.4000 - val_loss: 1.6595 - val_acc: 0.4225\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.77296 to 1.65948, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/150\n",
      "6838/6838 [==============================] - 11s 2ms/step - loss: 1.5615 - acc: 0.4345 - val_loss: 1.5584 - val_acc: 0.4807\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.65948 to 1.55840, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.4880 - acc: 0.4681 - val_loss: 1.4745 - val_acc: 0.5047\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.55840 to 1.47448, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/150\n",
      "6838/6838 [==============================] - 12s 2ms/step - loss: 1.4219 - acc: 0.4939 - val_loss: 1.4306 - val_acc: 0.4964\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.47448 to 1.43061, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 8/150\n",
      "3584/6838 [==============>...............] - ETA: 5s - loss: 1.3859 - acc: 0.5095"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-c00bec559d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training the model based on pretrained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-3f5c091116a1>\u001b[0m in \u001b[0;36mpip\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_building\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     history = model_training(n, model, train_features, train_labels, validate_features, \n\u001b[0;32m---> 20\u001b[0;31m                              validate_labels, num_epochs=150, num_batch_size=256)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# MobileNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8f29724cf455>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(n, model, train_features, train_labels, validate_features, validate_labels, num_epochs, num_batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#start = datetime.now()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#duration = datetime.now() - start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the model based on pretrained weights\n",
    "history, model = pip(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "#print(history.history.keys()) \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(933, 40, 173, 1)\n"
     ]
    }
   ],
   "source": [
    "pickle_file_path = 'audio_features_mfcc40.pickle'\n",
    "train_features, validate_features, test_features, train_labels,\\\n",
    "validate_labels, test_labels, y_one_hot = load_data(pickle_file_path)\n",
    "train_features = train_features.reshape( (train_features.shape[0], train_features.shape[1],  train_features.shape[2], 1))\n",
    "validate_features = validate_features.reshape((validate_features.shape[0], validate_features.shape[1],  validate_features.shape[2], 1))\n",
    "test_features = test_features.reshape((test_features.shape[0], test_features.shape[1],  test_features.shape[2], 1))\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6838/6838 [==============================] - 2s 288us/step\n",
      "Training Accuracy:  0.9226381982512974\n",
      "961/961 [==============================] - 0s 286us/step\n",
      "Validation Accuracy:  0.8751300728407908\n",
      "933/933 [==============================] - 0s 289us/step\n",
      "Test Accuracy:  0.8713826366559485\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "\n",
    "score = model.evaluate(train_features, train_labels, verbose=1)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(validate_features, validate_labels, verbose=1)\n",
    "print(\"Validation Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(test_features, test_labels, verbose=1)\n",
    "print(\"Test Accuracy: \", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Source: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# Save model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"sound_classification_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "# load json and create model\n",
    "json_file = open('sound_classification_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadoaded model\n",
    "loaded_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_feature() missing 1 required positional argument: 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-550fdbe837e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# make sure the sound file length is <= 2s, or doesn't contians info after 2s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_feature() missing 1 required positional argument: 'n'"
     ]
    }
   ],
   "source": [
    "# Test performance of user input audio file\n",
    "# x need to be numpy array of shape (40, 173) processed through mfcc\n",
    "# make sure the sound file length is <= 2s, or doesn't contians info after 2s\n",
    "file_path = ''\n",
    "x = extract_feature(file_path)\n",
    "print(x.shape)\n",
    "predictions = loaded_model.predict(x)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step:\n",
    "# 1. Run the model\n",
    "# 2. Save the data set (train/validation/test data)\n",
    "# 3. Save the model\n",
    "# 4. Improve model performance\n",
    "# 5. Modify the model - possible: MobileNet, VGG16, LeNet, GoogLeNet\n",
    "# 6. Predict the user input audio clip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
